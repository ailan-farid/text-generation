{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUGsKzNu1amS"
   },
   "source": [
    "# Mounting google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1162,
     "status": "ok",
     "timestamp": 1573697146922,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "-1RBXR841MBD",
    "outputId": "a8c0f7da-b931-40ec-eda7-f7fdca70a881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CFm-gAY1m2Y"
   },
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjIFbPRR1eAP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_5kYUpL13ZL"
   },
   "source": [
    "# Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1573697147544,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "jmjh80Cg14W4",
    "outputId": "8915cc65-9010-4b30-abfe-620829086e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4933 entries, 0 to 4932\n",
      "Data columns (total 3 columns):\n",
      "Year        4933 non-null int64\n",
      "Title       4933 non-null object\n",
      "Abstract    4933 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 115.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/text-generation/text-generation-word/nips.csv')\n",
    "df = df[df['Abstract'] != \"Abstract Missing\"]\n",
    "df = df.reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1573697147546,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "WzKP9TXw2CFy",
    "outputId": "4e18c388-1539-4d6f-d03e-2d98fd3e2f8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Up-\u0002propagation is an algorithm for inverting ...\n",
       "1    We have constructed an inexpensive video based...\n",
       "2    Non-negative matrix factorization (NMF) has pr...\n",
       "3    Spike-triggered averaging techniques are effec...\n",
       "4    We consider continuous state, continuous actio...\n",
       "Name: Abstract, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['Abstract']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1573697147777,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "0cSnuvj92WQj",
    "outputId": "337498dc-7c2a-4126-9875-f0c07247e54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words before text preprocessing: 732614\n",
      "Total number of unique words before text preprocessing:  41882\n",
      "['\"', '\"ALBO\"', '\"Air', '\"Air\\'\\'', '\"Answerer', '\"DIRECT\"', '\"DUOL\"', '\"Expansion-Constrained', '\"Generalized', '\"Ghost', '\"Graph', '\"GuessWhat?!\".', '\"Hedge\"', '\"Hey', '\"I', '\"Ising\\'\\'', '\"MNIST', '\"Object', '\"PixelGAN', '\"Self-Expressiveness', '\"Short-Dot\"', '\"TILT\\'\\'', '\"Ugliness-in-Averageness\"', '\"What', '\"additional', '\"anti-Bayesian\"', '\"autotags\")', '\"averagers,\"', '\"batch\"', '\"best', '\"body\\'\\'', '\"body\\'\\',', '\"building', '\"bus', '\"calibration', '\"catalyst\"', '\"chill\"', '\"comparison', '\"condition', '\"context\",', '\"convergence', '\"cooperative', '\"correctness\"', '\"date\"', '\"deep', '\"degrees', '\"deltas\",', '\"denoise\"', '\"describing\"', '\"determinantal', '\"disagreement', '\"disappearance\"', '\"discriminating\"', '\"discriminative', '\"early', '\"edge', '\"efficient\"', '\"em', '\"equalized', '\"exemplars\"', '\"expected', '\"extended', '\"external\"', '\"fair.\"', '\"few-shot\\'\\'', '\"filter', '\"follow-the-perturbed-leader\"', '\"found', '\"functional', '\"generalization', '\"generalized', '\"good\"', '\"graph-valued', '\"grown\"', '\"hard\"', '\"hiding\"', '\"hover', '\"human-like\"', '\"identity', '\"inherent\"', '\"intelligently\",', '\"interaction', '\"internal', '\"internal\"', '\"interrogators\",', '\"isotropic\"', '\"jogging\"', '\"kappa\"', '\"knows', '\"late', '\"latent\\'\\'', '\"latent\\'\\')', '\"learning', '\"local', '\"lucky\"', '\"manifold\"', '\"matching\"', '\"missingness\"', '\"multi-scale\"', '\"naive\"']\n",
      "['\"no\"', '\"none', '\"nonlinearity\"', '\"off-policyness\";', '\"optimal\"', '\"orientation\"', '\"overlapped\\'\\'', '\"partial', '\"patchily\",', '\"persistent', '\"pessimism', '\"pixel\"', '\"plain\"', '\"practice\"', '\"predictive', '\"privacy', '\"random', '\"reduced', '\"regression', '\"rendezvous\"', '\"s\"-sparse', '\"scaled', '\"self-expressiveness\"', '\"self-model\"', '\"shape\"of', '\"shared', '\"signature\"', '\"simple', '\"skeleton\"', '\"small', '\"small\"', '\"smoothed', '\"spectral', '\"standard\"', '\"status', '\"stragglers\"', '\"structured\",', '\"student\"', '\"successor', '\"surprise\",', '\"suspense\",', '\"targeted\"', '\"throw', '\"training', '\"travel\"', '\"trending', '\"two', '\"two-phase\\'\\'', '\"two-player', '\"typical', '\"ultra-slow\"', '\"universal\\'\\'', '\"visual', '\"volume', '\"walk', '\"what', '\"where', '\"world\"', '\"world-model\"', '#101.', '#86', '#P', '#P-hard', '#P-hard,', '#P-hard.', '#example', '$', '$(+\\\\!3,', '$(0,\\\\pi/2]$.', '$(1', '$(1+(1+\\\\epsilon)\\\\gamma)$-approximate', '$(1+O(\\\\psi))$-approximation', '$(1+\\\\alpha)\\\\,L^*_\\\\gamma', '$(1+\\\\eps)$', '$(1+\\\\eps)$-approximation', '$(1+\\\\epsilon)$', '$(1+\\\\epsilon)$-approximation', '$(1+\\\\epsilon)$-factor', '$(1+\\\\epsilon)z$,', '$(1+\\\\varepsilon)$', '$(1-1/\\\\sqrt{\\\\kappa})$', '$(1-1/e)$', '$(1-1/e)-\\\\epsilon$', '$(1-1/e)^2-\\\\delta$', '$(1-\\\\eps)f(S)', '$(1-\\\\epsilon)$-approximation', '$(1-\\\\varepsilon)^{\\\\ell}(1-1/e)$', '$(1/\\\\sqrt{n})$', '$(1/n)$', '$(1\\\\pm\\\\eps)$-coreset.', '$(2+\\\\epsilon)$-approximation', '$(2,', '$(2,1)$-norm', '$(3+o(1))/n^{1/3}$', '$(3,', '$(Christopher,', '$(L', '$(O(\\\\log^{-1}', '$(X_1,', '$(\\\\alpha,']\n",
      "['$(\\\\beta,B)$-Bernstein,', '$(\\\\delta,\\\\rho)$-mode', '$(\\\\delta,\\\\rho)$-modes', '$(\\\\ell,', '$(\\\\ell,\\\\mathcal{F},', '$(\\\\epsilon,', '$(\\\\epsilon,\\\\gamma)$-SOSP.', '$(\\\\epsilon,\\\\gamma)$-second', '$(\\\\frac{\\\\text{OPT}}{2}-\\\\epsilon)$.', '$(\\\\infty,', '$(\\\\varepsilon,', '$(d+1)$-partite', '$(d+1)n$', '$(has\\\\_husband,', '$(i)$', '$(i,j)$', '$(ii)$', '$(iii)$', '$(iv)$', '$(k+1)$-th', '$(k,', '$(m\\\\gg', '$(s,a)$,', '$(x_k)_{k=0}^K$,', '$(x_k)_{k=1}^K$', '$(y_k', '$+$', '$+1$', '$+1/\\\\sqrt{t}$', '$+\\\\!3$', '$-$in', '$-1$', '$-1/\\\\sqrt{t}$', '$0', '$0$', '$0$,', '$0.408', '$1', '$1$', '$1$)', '$1$,', '$1$-bit', '$1$-dimensional', '$1$-norm', '$1$.', '$1)$', '$1+\\\\Omega(\\\\frac{\\\\ln^2', '$1+\\\\epsilon$,', '$1+o(1)$.', '$1,', '$1,\\\\!000\\\\times$', '$1,\\\\infty$', '$1,\\\\infty$-regularized', '$1-1/e$', '$1-\\\\alpha$.', '$1-\\\\alpha_i$', '$1-\\\\delta$', '$1-\\\\delta$,', '$1-\\\\delta$.', '$1-c/e$', '$1-e^{-\\\\Omega(M)}$.', '$1.6\\\\%$', '$1/', '$1/(1-2\\\\eta)$,', '$1/2$', '$1/L$,', '$1/T$', '$1/\\\\alpha$', '$1/\\\\epsilon$', '$1/\\\\epsilon$.', '$1/\\\\sqrt{T}$', '$1/\\\\sqrt{n}$', '$1/\\\\sqrt{t}$', '$1/k$.', '$1/n^{\\\\gamma/c}$', '$1/p+1/p^{*}=1$,', '$1/t^2$', '$10$', '$10$.', '$10-20\\\\%$', '$100,\\\\!000$s', '$10\\\\times', '$10^4$.', '$10^8$.', '$10^{-12}$', '$10^{-3}$', '$10^{-6}$', '$10^{64}$', '$10^{70}$', '$11\\\\%$', '$121', '$13\\\\%$.', '$14', '$14\\\\%$', '$14\\\\epsilon', '$160', '$1\\\\le', '$1\\\\leq', '$2', '$2$.']\n"
     ]
    }
   ],
   "source": [
    "corpus_init = ' '.join(list(text))\n",
    "words_init = corpus_init.split()\n",
    "n_words_init = len(words_init)\n",
    "unique_words_init = sorted(list(set(words_init)))\n",
    "n_unique_words_init = len(unique_words_init)\n",
    "print(\"Total number of words before text preprocessing:\", n_words_init)\n",
    "print(\"Total number of unique words before text preprocessing: \", n_unique_words_init)\n",
    "print(unique_words_init[:100])\n",
    "print(unique_words_init[100:200])\n",
    "print(unique_words_init[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gWgjLvI2oz7"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeLFju5c2sMk"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word != ',' or word != '.':\n",
    "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "          new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD8tTIFw2vDc"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # noise removal\n",
    "  text = re.sub(r'\\bhttps?://\\w+.+[^ ]\\b', 'link', text) #replace url with \"link\"\n",
    "  text = re.sub(r'[a-zA-Z0-9]*\\.?github\\.?[a-zA-Z0-9]*','github',text) #replace github links with \"github\"\n",
    "  text = re.sub(r'\\~\\\\cite\\{[^}]*\\}','cite',text) # remove cite in the format of \"~\\cite{DeSaOR16}\"\n",
    "  text = re.sub(r'\\[[^]]*\\]', '', text) # remove between square brackets\n",
    "  text = re.sub(r'\\([^)]*\\)', '', text) # remove between parentheses\n",
    "  text = re.sub(r'\\{[^)]*\\}', '', text) # remove between curly brackets\n",
    "  \n",
    "  # normalization\n",
    "  text = text.lower() # convert to lowercase text\n",
    "  \n",
    "  text = re.sub(r'\\-',' ', text) # seperate words like 'video-related'\n",
    "  text = re.sub(r'\\/',' ', text) # seperate words like 'descriptors/tags'\n",
    "  text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,]', '', text) # remove punctuation \n",
    "  text = re.sub(r'seq2seq', 'seqtoseq', text)\n",
    "  \n",
    "  text = re.sub(r'[-+]?\\d*\\.?\\d+', 'NUMBER', text) # replace numbers with \"NUMBER\"\n",
    "  \n",
    "  text = re.sub(r'\\.{2,}', '', text) # remove '..','...'\n",
    "  text = re.sub(r'\\.', ' . ', text) # seperate '.' from text\n",
    "  text = re.sub(r'\\,' , ' , ', text) # seperate ',' from text\n",
    " \n",
    "  text = ' '.join(remove_non_ascii(text.split())) # remove non-ascii words\n",
    "  text = re.sub(r'[\\w]*NUMBER[\\w]*', 'NUMBER', text) # replace anyword containing \"NUMBER\" with \"NUMBER\"\n",
    "\n",
    "  # convert from British English to American English\n",
    "  text = re.sub(r'modelled', 'modeled', text) \n",
    "  text = re.sub(r'modelling', 'modeling', text)\n",
    "  text = re.sub(r'parallelisation', 'parallelization', text) \n",
    "  text = re.sub(r'parallelising', 'parallelizing', text)\n",
    "  text = re.sub(r'analysed', 'analyzed', text) \n",
    "  text = re.sub(r'generalised', 'generalized', text) \n",
    "  text = re.sub(r'maximisation', 'maximization', text)\n",
    "  text = re.sub(r'recogniser', 'recognizer', text)\n",
    "  text = re.sub(r'optimised', 'optimized', text)\n",
    "  text = re.sub(r'analyse', 'analyze', text)\n",
    "  text = re.sub(r'generalisation', 'generalization', text)\n",
    "  text = re.sub(r'generalised', 'generalized', text)\n",
    "  text = re.sub(r'factorisation', 'factorization', text)\n",
    "  text = re.sub(r'behaviour', 'behavior', text)\n",
    "  text = re.sub(r'interpretted', 'interpreted', text)\n",
    "  text = re.sub(r'neighbouring', 'neighboring', text)\n",
    "  text = re.sub(r'neighbour', 'neighbor', text)\n",
    "  text = re.sub(r'neighbours', 'neighbors', text)\n",
    "  text = re.sub(r'dependant', 'dependent', text)\n",
    "  text = re.sub(r'localisation', 'localization', text)\n",
    "  text = re.sub(r'amortised', 'amortized', text)\n",
    "  text = re.sub(r'amortisation', 'amortization', text)\n",
    "  text = re.sub(r'neutralising', 'neutralizing', text)\n",
    "  text = re.sub(r'prioritised', 'prioritized', text)\n",
    "  text = re.sub(r'characterised', 'characterized', text)\n",
    "  text = re.sub(r'characterise', 'characterize', text)\n",
    "  text = re.sub(r'centeralised', 'centeralized',text)\n",
    "  text = re.sub(r'initialisation', 'initialization', text)\n",
    "  text = re.sub(r'initialised', 'initialized', text)\n",
    "  text = re.sub(r'regularisation', 'regularization', text)\n",
    "  text = re.sub(r'regularised', 'regularized', text)\n",
    "  text = re.sub(r'optimisation', 'optimization', text)\n",
    "  text = re.sub(r'optimise', 'optimize', text)\n",
    "  text = re.sub(r'minimisation', 'minimization', text)\n",
    "  text = re.sub(r'generalises', 'generalizes', text)\n",
    "  text = re.sub(r'parameterised', 'parameterized', text)\n",
    "  text = re.sub(r'parameterises', 'parameterizes', text)\n",
    "  text = re.sub(r'reparameterisation', 'reparameterization', text)\n",
    "  text = re.sub(r'optimising', 'optimizing', text)\n",
    "  text = re.sub(r'favourable', 'favorable', text)\n",
    "  text = re.sub(r'hypothesised', 'hypothesized', text)\n",
    "  text = re.sub(r'summarise', 'summarize', text)\n",
    "  text = re.sub(r'standardised', 'standardized', text)\n",
    "  text = re.sub(r'randomisation', 'randomization', text)\n",
    "  text = re.sub(r'synchronisation', 'synchronization', text)\n",
    "  text = re.sub(r'travelling', 'traveling', text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehFn-2ZG21g1"
   },
   "outputs": [],
   "source": [
    "text = text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8201,
     "status": "ok",
     "timestamp": 1573697154105,
     "user": {
      "displayName": "Yuan Hu",
      "photoUrl": "",
      "userId": "14204862805009818583"
     },
     "user_tz": 480
    },
    "id": "eIAhcAma2-FE",
    "outputId": "43186f39-7420-4f86-cafa-a850ff9370d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 784009\n",
      "Total number of unique words:  15032\n",
      "[',', '.', 'NUMBER', 'a', 'aa', 'aaai', 'aalen', 'aaronson', 'ab', 'abandons', 'abbe', 'abc', 'abdominal', 'aberrant', 'abf', 'abilities', 'ability', 'ablation', 'able', 'abnormal', 'abnormalities', 'abnormality', 'abound', 'abounds', 'about', 'above', 'abovethreshold', 'abp', 'abrupt', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbing', 'absorption', 'abstain', 'abstaining', 'abstention', 'abstract', 'abstracted', 'abstraction', 'abstractions', 'abstractive', 'abstracts', 'abundance', 'abundancy', 'abundant', 'abuse', 'ac', 'academic', 'academics', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerators', 'accelerometers', 'accentuated', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'accepted', 'accepts', 'access', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accident', 'accidental', 'accidents', 'acclaimed', 'accnet', 'accommodate', 'accommodated', 'accommodates', 'accompanied', 'accompany', 'accompanying', 'accomplish', 'accomplished', 'accomplishes', 'accomplishing', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountable', 'accounted', 'accounting', 'accounts', 'accp']\n",
      "['accross', 'accumulate', 'accumulated', 'accumulating', 'accumulation', 'accumulations', 'accumulator', 'accumulators', 'accuracies', 'accuracy', 'accurate', 'accurately', 'acdm', 'ace', 'acetylcholine', 'achievability', 'achievable', 'achieve', 'achieved', 'achievements', 'achieves', 'achieving', 'achromatic', 'acid', 'acids', 'acknowledge', 'acknowledged', 'acknowledges', 'acnn', 'acnns', 'acoustic', 'acoustics', 'acquire', 'acquired', 'acquires', 'acquiring', 'acquisition', 'acquisitions', 'acronym', 'across', 'acrossa', 'acs', 'act', 'acted', 'acterization', 'acting', 'action', 'actionable', 'actions', 'activate', 'activated', 'activates', 'activating', 'activation', 'activations', 'active', 'actively', 'activities', 'activity', 'actor', 'actors', 'acts', 'actual', 'actually', 'actuated', 'actuation', 'acute', 'acutely', 'acyclic', 'acyclicity', 'ad', 'ada', 'adaboost', 'adadelta', 'adagan', 'adagrad', 'adam', 'adams', 'adamsmoulton', 'adapt', 'adaptability', 'adaptation', 'adaptations', 'adapted', 'adapter', 'adapting', 'adaption', 'adaptive', 'adaptively', 'adaptiveness', 'adaptivity', 'adaptor', 'adapts', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additions']\n",
      "['additive', 'additivity', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'ade', 'adept', 'adequacy', 'adequate', 'adequately', 'ader', 'adheres', 'adhering', 'adjacency', 'adjacent', 'adjusments', 'adjust', 'adjustable', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adjusts', 'adm', 'administered', 'admira', 'admissibility', 'admissible', 'admission', 'admissions', 'admit', 'admits', 'admitting', 'admixture', 'admixtures', 'admm', 'adni', 'ado', 'adolescent', 'adopt', 'adopted', 'adopting', 'adoption', 'adopts', 'adp', 'adpp', 'adress', 'adresses', 'ads', 'adults', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantageous', 'advantages', 'advection', 'advent', 'adversarial', 'adversarially', 'adversaries', 'adversary', 'adversarys', 'adverse', 'adversely', 'advertise', 'advertisement', 'advertisements', 'advertiser', 'advertisers', 'advertising', 'advi', 'advice', 'advise', 'advised', 'advocate', 'advocated', 'adwords', 'aen', 'aer', 'aerial', 'aesthetic', 'aesthetically', 'aesthetics', 'afd', 'affairs', 'affect', 'affected', 'affecting', 'affects', 'afferent', 'afferents', 'affiliation', 'affine', 'affinities']\n",
      "['affinity', 'affirm', 'affirmation', 'affirmative', 'affirmatively', 'afford', 'affordable', 'afforded', 'affording', 'affords', 'afhmm', 'aforementioned', 'aforesaid', 'african', 'after', 'aftern', 'afterwards', 'ag', 'again', 'against', 'age', 'agency', 'agenda', 'agendas', 'agent', 'agents', 'agglomerate', 'agglomeration', 'agglomerations', 'agglomerative', 'aggregate', 'aggregated', 'aggregates', 'aggregating', 'aggregation', 'aggregations', 'aggregative', 'aggressive', 'aggressively', 'agm', 'agnos', 'agnostic', 'agnostically', 'agnostophobia', 'ago', 'agorithm', 'agree', 'agreement', 'agrees', 'ags', 'ahead', 'ahp', 'ahundred', 'ai', 'aic', 'aid', 'aide', 'aided', 'aiding', 'aids', 'aim', 'aimathbf', 'aimbrain', 'aimed', 'aiming', 'aims', 'air', 'airborne', 'aircraft', 'airflow', 'airline', 'ais', 'ait', 'ak', 'aka', 'akaike', 'akin', 'al', 'alarm', 'alarmingly', 'alarms', 'albeit', 'albo', 'album', 'alcohol', 'aldous', 'ale', 'aleatoric', 'alert', 'alexanders', 'alexnet', 'algebra', 'algebraic', 'algebraically', 'algoirthm', 'algorithm', 'algorithmic', 'algorithmically', 'algorithms', 'aliasing']\n",
      "['alice', 'aligment', 'align', 'aligned', 'aligning', 'alignment', 'alignments', 'aligns', 'alike', 'all', 'allay', 'allegedly', 'allen', 'alleviate', 'alleviated', 'alleviates', 'alleviating', 'alligned', 'allocate', 'allocated', 'allocates', 'allocating', 'allocation', 'allocations', 'allow', 'allowable', 'allowed', 'allowing', 'allows', 'alloy', 'allreduce', 'almost', 'alone', 'along', 'alongside', 'aloocv', 'alp', 'alpaca', 'alpha', 'alphabet', 'alphabets', 'alphacsc', 'alphago', 'alphai', 'alphain', 'already', 'als', 'also', 'alter', 'alteration', 'alterations', 'altered', 'altering', 'alternate', 'alternated', 'alternately', 'alternates', 'alternating', 'alternation', 'alternations', 'alternative', 'alternatively', 'alternatives', 'altest', 'although', 'altitude', 'altogether', 'always', 'alzheimer', 'alzheimers', 'am', 'amalgamates', 'amazing', 'amazon', 'ambient', 'ambiguities', 'ambiguity', 'ambiguous', 'ambitious', 'ambitiously', 'amenability', 'amenable', 'amend', 'amendable', 'amended', 'american', 'amini', 'amino', 'ammar', 'amoebe', 'among', 'amongst', 'amortization', 'amortize', 'amortized', 'amortizes', 'amount', 'amounts', 'amp', 'ample']\n",
      "['amplification', 'amplified', 'amplify', 'amplifying', 'amplitude', 'amplitudes', 'an', 'analog', 'analogical', 'analogies', 'analogous', 'analogously', 'analogs', 'analogue', 'analogues', 'analogy', 'analysing', 'analysis', 'analyst', 'analysts', 'analytic', 'analytical', 'analytically', 'analyticity', 'analytics', 'analyze', 'analyzed', 'analyzer', 'analyzers', 'analyzes', 'analyzing', 'anarchy', 'anatomic', 'anatomical', 'anatomically', 'anatomy', 'ancestor', 'ancestral', 'anchor', 'anchored', 'anchoring', 'anchors', 'ancient', 'ancillary', 'and', 'anderson', 'andersson', 'android', 'anecdotal', 'anechoic', 'anencoder', 'anesthesia', 'anesthetized', 'anew', 'angle', 'angles', 'angluins', 'angular', 'animal', 'animals', 'animated', 'animation', 'anisotropic', 'anisotropism', 'anlysis', 'anms', 'ann', 'anneal', 'annealed', 'annealing', 'annotate', 'annotated', 'annotating', 'annotation', 'annotations', 'annotator', 'annotators', 'announced', 'anns', 'annual', 'anomalies', 'anomalous', 'anomaly', 'anonymity', 'anonymize', 'another', 'anothers', 'anova', 'answer', 'answerer', 'answerers', 'answering', 'answers', 'ant', 'antagonistic', 'ante', 'anteed', 'antemakes', 'antennal', 'anterior']\n",
      "['anthropomorphic', 'anti', 'anticipate', 'anticipated', 'anticipation', 'anticipative', 'anticipatory', 'antisense', 'antithetic', 'ants', 'any', 'anymore', 'anything', 'anytime', 'anyway', 'aois', 'ap', 'apache', 'apart', 'apascal', 'apcg', 'aperture', 'apg', 'api', 'apical', 'apm', 'aposteriori', 'app', 'apparatus', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appears', 'append', 'appled', 'appli', 'appliance', 'appliances', 'applica', 'applicability', 'applicable', 'applicant', 'applicants', 'application', 'applications', 'applicationshave', 'applicationsin', 'applicationsneural', 'applied', 'applies', 'apply', 'applying', 'appreciate', 'appreciated', 'apprentices', 'apprenticeship', 'approach', 'approached', 'approaches', 'approachessatisfying', 'approaching', 'approachs', 'approch', 'approporiate', 'appropriate', 'appropriately', 'appropriateness', 'appropriation', 'approval', 'approved', 'approving', 'approx', 'approximable', 'approximant', 'approximate', 'approximated', 'approximately', 'approximates', 'approximating', 'approximation', 'approximations', 'approximator', 'approximators', 'apropensity', 'apy', 'aqm', 'aquadratic', 'aquatic', 'ar', 'arabic', 'aracade', 'arate', 'arbiters', 'arbitrage', 'arbitrarily']\n"
     ]
    }
   ],
   "source": [
    "text_list = list(text)\n",
    "corpus = ' '.join(text_list)\n",
    "words = corpus.split()\n",
    "n_words = len(words)\n",
    "unique_words = sorted(list(set(words)))\n",
    "n_unique_words = len(unique_words)\n",
    "print(\"Total number of words:\", n_words)\n",
    "print(\"Total number of unique words: \", n_unique_words)\n",
    "print(unique_words[:100])\n",
    "print(unique_words[100:200])\n",
    "print(unique_words[200:300])\n",
    "print(unique_words[300:400])\n",
    "print(unique_words[400:500])\n",
    "print(unique_words[500:600])\n",
    "print(unique_words[600:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save clean text to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFg2eD8G3PcE"
   },
   "outputs": [],
   "source": [
    "# save clean text to .txt for model training\n",
    "text_str = '\\n'.join(text_list)\n",
    "with open(\"nips_clean.txt\", 'w') as file:\n",
    "  file.write(text_str)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
